# ğŸ§  MCP Course

Este projeto demonstra como integrar o protocolo **MCP (Model Context Protocol)** com agentes LLM utilizando **Google Gemini** e a biblioteca **LangChain**, permitindo chamadas de ferramentas (tools) executadas localmente por meio de um servidor MCP.

## ğŸ“¦ VisÃ£o Geral

O projeto possui duas abordagens:

### Explorar o MCP + Gemini:
* `client.py`: Cliente MCP que se conecta a um servidor via stdio e utiliza diretamente o SDK do **Google Gemini** para gerar respostas e acionar ferramentas.

### Explorar MCP com LangChain + LangGraph + Gemini:
* `langchain_mcp_client.py`: Variante que usa **LangChain** e **LangGraph** para criar um agente ReAct com ferramentas MCP integradas, tambÃ©m utilizando o modelo Gemini.

### ğŸ“ Estrutura

```
MCP-CLIENT/
â”‚
â”œâ”€â”€ assets/                 # Imagens dos resultados
â”œâ”€â”€ output/                 # SaÃ­da dos comandos executados pelo servidor MCP
â”‚
â”œâ”€â”€ .env_example            # Exemplo de como montar seu .env
â”‚
â”œâ”€â”€ client.py               # Cliente MCP que utiliza diretamente a API do Gemini
â”œâ”€â”€ langchain_mcp_client.py # Cliente com LangChain + LangGraph + Gemini
â”œâ”€â”€ terminal_server.py      # Servidor MCP com ferramenta para execuÃ§Ã£o de comandos
â”‚
â”œâ”€â”€ pyproject.toml          # ConfiguraÃ§Ãµes do projeto e dependÃªncias
â””â”€â”€ README.md
```

---

## âš™ï¸ PrÃ©-requisitos

* Python 3.10+
* ``uv`` instalado: 
No MacOS/Linux:
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh 
``` 
No Windows: siga as instruÃ§Ãµes de instalaÃ§Ã£o na [pÃ¡gina](https://docs.astral.sh/uv/getting-started/installation/#__tabbed_1_2)
* Chave de API do Google Gemini (obtida via [Google AI Studio](https://makersuite.google.com/))

---

## ğŸ“¦ InstalaÃ§Ã£o

1. Clone este repositÃ³rio:

```bash
git clone ...
cd ...
```

2. Ative o ambiente virtual:
No MacOS/Linux:
```bash
uv venv
source .venv/bin/activate
```
No Windows:
```bash
uv venv
.venv\Scripts\activate  
```

2. Instale as dependÃªncias com o UV:

```bash
uv pip install -r pyproject.toml
```
ou
```bash
uv pip install .
```

3. Crie um arquivo `.env` na raiz com a seguinte variÃ¡vel:

```
GEMINI_API_KEY=sua_chave_api_gemini
```

---

## ğŸš€ Executando

### 1. Inicie o cliente tradicional com Gemini:

Este cliente usa diretamente o SDK do Gemini para responder e interagir com o servidor MCP:

```bash
uv run client.py terminal_server.py
```

### 2. Ou use o cliente com LangChain:

Este cliente cria um agente ReAct com LangChain + LangGraph:

```bash
uv run langchain_mcp_client.py terminal_server.py
```

### Resultados
Conforme mostra no ``output/``, foram criados arquivos .txt demonstrando as interaÃ§Ãµes bem-sucedidas entre um cliente LLM (como Gemini) e um servidor MCP.

![Resultado mcp + gemini](./assets/mcp_gemini.png)
![Resultado lanchain + gemini](./assets/langgraph_mcp_gemini.png)